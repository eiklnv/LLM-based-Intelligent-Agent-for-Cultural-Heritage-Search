"""
å¤–éƒ¨å·¥å…·é›†æˆæ¨¡å—
"""
import requests
import json
import logging
from typing import Dict, List, Any, Optional
from urllib.parse import urlencode, quote
from bs4 import BeautifulSoup
import re
from langchain.tools import Tool
from langchain.pydantic_v1 import BaseModel, Field


class SearchResult(BaseModel):
    """æœç´¢ç»“æœæ•°æ®æ¨¡å‹"""
    title: str = Field(description="é¡µé¢æ ‡é¢˜")
    url: str = Field(description="é¡µé¢URL")
    content: str = Field(description="é¡µé¢å†…å®¹æ‘˜è¦")
    score: float = Field(description="ç›¸å…³æ€§è¯„åˆ†", default=0.0)
    source: str = Field(description="ä¿¡æ¯æ¥æº", default="")


class SearxngTool:
    """SearXNGæœç´¢å·¥å…·"""
    
    def __init__(self, base_url: str = "http://localhost:8888", 
                 language: str = "zh-CN", timeout: int = 10):
        """
        åˆå§‹åŒ–SearXNGæœç´¢å·¥å…·
        
        Args:
            base_url: SearXNGæœåŠ¡åœ°å€
            language: æœç´¢è¯­è¨€
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.base_url = base_url.rstrip('/')
        self.language = language
        self.timeout = timeout
        self.session = requests.Session()
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'RelicSeek/1.0 (Cultural Heritage Search System)',
            'Accept': 'application/json, text/html, */*',
            'Accept-Language': language
        })
    
    def search(self, query: str, categories: List[str] = None, 
               engines: List[str] = None, max_results: int = 20) -> List[SearchResult]:
        """
        æ‰§è¡Œæœç´¢æŸ¥è¯¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢è¯
            categories: æœç´¢åˆ†ç±»ï¼Œå¦‚['general', 'images', 'news']
            engines: æœç´¢å¼•æ“ï¼Œå¦‚['bing', 'google', 'duckduckgo']
            max_results: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # æ‰“å°æœç´¢è°ƒè¯•ä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ” å¼€å§‹æ‰§è¡ŒSearXNGæœç´¢")
            self.logger.info(f"ğŸ“ æœç´¢å…³é”®è¯: {query}")
            self.logger.info(f"ğŸŒ æœç´¢è¯­è¨€: {self.language}")
            self.logger.info(f"ğŸ“‚ æœç´¢åˆ†ç±»: {categories or ['general']}")
            self.logger.info(f"ğŸ”§ æœç´¢å¼•æ“: {engines or ['default']}")
            self.logger.info(f"ğŸ“Š æœ€å¤§ç»“æœæ•°: {max_results}")
            self.logger.info(f"ğŸŒ æœåŠ¡åœ°å€: {self.base_url}")
            
            # æ„å»ºæœç´¢å‚æ•°
            params = {
                'q': query,
                'format': 'json',
                'language': self.language,
                'safesearch': '1'
            }
            
            if categories:
                params['categories'] = ','.join(categories)
            if engines:
                params['engines'] = ','.join(engines)
            
            self.logger.info(f"ğŸ“‹ æœç´¢å‚æ•°: {params}")
            
            # å‘é€æœç´¢è¯·æ±‚
            search_url = f"{self.base_url}/search"
            self.logger.info(f"ğŸš€ å‘é€è¯·æ±‚åˆ°: {search_url}")
            
            response = self.session.get(search_url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            self.logger.info(f"âœ… è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            
            # è§£ææœç´¢ç»“æœ
            data = response.json()
            raw_results = data.get('results', [])
            
            self.logger.info(f"ğŸ“¥ æ”¶åˆ°åŸå§‹ç»“æœæ•°é‡: {len(raw_results)}")
            
            results = []
            
            for i, item in enumerate(raw_results[:max_results]):
                result = SearchResult(
                    title=item.get('title', ''),
                    url=item.get('url', ''),
                    content=item.get('content', ''),
                    score=self._calculate_relevance_score(item, query),
                    source=self._extract_source_domain(item.get('url', ''))
                )
                results.append(result)
                
                # æ‰“å°æ¯ä¸ªæœç´¢ç»“æœçš„è¯¦ç»†ä¿¡æ¯
                self.logger.info(f"ğŸ“„ ç»“æœ {i+1}:")
                self.logger.info(f"   ğŸ“Œ æ ‡é¢˜: {result.title[:100]}{'...' if len(result.title) > 100 else ''}")
                self.logger.info(f"   ğŸ”— URL: {result.url}")
                self.logger.info(f"   ğŸ“ å†…å®¹æ‘˜è¦: {result.content[:150]}{'...' if len(result.content) > 150 else ''}")
                self.logger.info(f"   â­ ç›¸å…³æ€§è¯„åˆ†: {result.score:.2f}")
                self.logger.info(f"   ğŸ¢ æ¥æº: {result.source}")
            
            # æŒ‰ç›¸å…³æ€§æ’åº
            results.sort(key=lambda x: x.score, reverse=True)
            
            self.logger.info(f"ğŸ¯ æœ€ç»ˆè¿”å›ç»“æœæ•°é‡: {len(results)}")
            self.logger.info("=" * 60)
            
            return results
            
        except requests.RequestException as e:
            self.logger.error(f"âŒ SearXNGæœç´¢å¤±è´¥: {str(e)}")
            raise Exception(f"SearXNGæœç´¢å¤±è´¥: {str(e)}")
        except json.JSONDecodeError:
            self.logger.error("âŒ SearXNGè¿”å›æ•°æ®æ ¼å¼é”™è¯¯")
            raise Exception("SearXNGè¿”å›æ•°æ®æ ¼å¼é”™è¯¯")
        except Exception as e:
            self.logger.error(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            raise Exception(f"æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    
    def _calculate_relevance_score(self, item: Dict[str, Any], query: str) -> float:
        """è®¡ç®—æœç´¢ç»“æœçš„ç›¸å…³æ€§è¯„åˆ†"""
        score = 0.0
        query_terms = set(query.lower().split())
        
        # æ ‡é¢˜åŒ¹é…åº¦
        title = item.get('title', '').lower()
        title_matches = sum(1 for term in query_terms if term in title)
        score += title_matches * 0.4
        
        # å†…å®¹åŒ¹é…åº¦
        content = item.get('content', '').lower()
        content_matches = sum(1 for term in query_terms if term in content)
        score += content_matches * 0.3
        
        # URLåŒ¹é…åº¦
        url = item.get('url', '').lower()
        url_matches = sum(1 for term in query_terms if term in url)
        score += url_matches * 0.1
        
        # æƒå¨æ€§åŠ æƒ
        source_bonus = self._get_source_authority_bonus(item.get('url', ''))
        score += source_bonus
        
        return min(score, 5.0)  # é™åˆ¶æœ€é«˜åˆ†ä¸º5åˆ†
    
    def _get_source_authority_bonus(self, url: str) -> float:
        """æ ¹æ®ä¿¡æ¯æºçš„æƒå¨æ€§ç»™äºˆåŠ åˆ†"""
        if not url:
            return 0.0
        
        url = url.lower()
        
        # å®˜æ–¹åšç‰©é¦†å’Œæ–‡åŒ–æœºæ„
        if any(domain in url for domain in ['museum', 'gov.cn', 'palace', 'cultural']):
            return 1.0
        
        # å­¦æœ¯æœºæ„å’Œæ•™è‚²ç½‘ç«™
        if any(domain in url for domain in ['edu.cn', 'academic', 'university', 'scholar']):
            return 0.8
        
        # æƒå¨ç™¾ç§‘ç½‘ç«™
        if any(domain in url for domain in ['baike.baidu.com', 'wikipedia', 'britannica']):
            return 0.6
        
        # ä¸“ä¸šæ–‡åŒ–åª’ä½“
        if any(domain in url for domain in ['cul.cn', 'wenhua', 'heritage']):
            return 0.4
        
        return 0.0
    
    def _extract_source_domain(self, url: str) -> str:
        """æå–ä¿¡æ¯æºåŸŸå"""
        if not url:
            return "æœªçŸ¥æ¥æº"
        
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc
            
            # ç®€åŒ–åŸŸåæ˜¾ç¤º
            if domain.startswith('www.'):
                domain = domain[4:]
            
            return domain
        except:
            return "æœªçŸ¥æ¥æº"


class WebContentExtractor:
    """ç½‘é¡µå†…å®¹æå–å™¨"""
    
    def __init__(self, timeout: int = 10):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'RelicSeek/1.0 (Cultural Heritage Search System)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
        })
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
    
    def extract_content(self, url: str, max_length: int = 5000) -> Dict[str, str]:
        """
        æå–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            max_length: æœ€å¤§å†…å®¹é•¿åº¦
            
        Returns:
            åŒ…å«æ ‡é¢˜ã€æ­£æ–‡å†…å®¹çš„å­—å…¸
        """
        try:
            # æ‰“å°å†…å®¹æå–è°ƒè¯•ä¿¡æ¯
            self.logger.info("=" * 50)
            self.logger.info("ğŸ“„ å¼€å§‹æå–ç½‘é¡µå†…å®¹")
            self.logger.info(f"ğŸ”— ç›®æ ‡URL: {url}")
            self.logger.info(f"ğŸ“ æœ€å¤§å†…å®¹é•¿åº¦: {max_length}")
            
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            
            self.logger.info(f"âœ… ç½‘é¡µè¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            self.logger.info(f"ğŸ“ å“åº”ç¼–ç : {response.encoding}")
            self.logger.info(f"ğŸ“Š å“åº”å†…å®¹é•¿åº¦: {len(response.text)} å­—ç¬¦")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for tag in soup(['script', 'style', 'nav', 'footer', 'aside']):
                tag.decompose()
            
            # æå–æ ‡é¢˜
            title = ""
            if soup.title:
                title = soup.title.get_text().strip()
            
            self.logger.info(f"ğŸ“Œ æå–åˆ°æ ‡é¢˜: {title[:100]}{'...' if len(title) > 100 else ''}")
            
            # æå–æ­£æ–‡å†…å®¹
            content = ""
            
            # ä¼˜å…ˆå¯»æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|article'))
            
            if main_content:
                content = main_content.get_text()
                self.logger.info("ğŸ¯ ä½¿ç”¨ä¸»è¦å†…å®¹åŒºåŸŸæå–å†…å®¹")
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œåˆ™æå–bodyä¸­çš„æ–‡æœ¬
                body = soup.find('body')
                if body:
                    content = body.get_text()
                    self.logger.info("ğŸ“„ ä½¿ç”¨bodyåŒºåŸŸæå–å†…å®¹")
                else:
                    content = soup.get_text()
                    self.logger.info("ğŸŒ ä½¿ç”¨æ•´ä¸ªé¡µé¢æå–å†…å®¹")
            
            # æ¸…ç†å†…å®¹
            original_length = len(content)
            content = self._clean_content(content)
            cleaned_length = len(content)
            
            self.logger.info(f"ğŸ§¹ å†…å®¹æ¸…ç†: {original_length} -> {cleaned_length} å­—ç¬¦")
            
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > max_length:
                content = content[:max_length] + "..."
                self.logger.info(f"âœ‚ï¸ å†…å®¹æˆªæ–­åˆ° {max_length} å­—ç¬¦")
            
            self.logger.info(f"ğŸ“‹ æœ€ç»ˆå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            self.logger.info("=" * 50)
            
            return {
                'title': title,
                'content': content,
                'url': url
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å†…å®¹æå–å¤±è´¥: {str(e)}")
            return {
                'title': f"å†…å®¹æå–å¤±è´¥: {str(e)}",
                'content': "",
                'url': url
            }
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†æå–çš„å†…å®¹"""
        if not content:
            return ""
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[^\w\s\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', ' ', content)
        
        # ç§»é™¤è¿‡çŸ­çš„è¡Œ
        lines = content.split('\n')
        cleaned_lines = [line.strip() for line in lines if len(line.strip()) > 10]
        
        return '\n'.join(cleaned_lines).strip()


class RelicSearchToolkit:
    """æ–‡ç‰©æœç´¢å·¥å…·åŒ…"""
    
    def __init__(self, searxng_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å·¥å…·åŒ…
        
        Args:
            searxng_config: SearXNGé…ç½®
        """
        self.searxng = SearxngTool(
            base_url=searxng_config.get('base_url', 'http://localhost:8888'),
            language=searxng_config.get('language', 'zh-CN'),
            timeout=searxng_config.get('timeout', 10)
        )
        self.extractor = WebContentExtractor()
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # é»˜è®¤æœç´¢å‚æ•°
        self.default_categories = searxng_config.get('categories', ['general'])
        self.default_engines = searxng_config.get('engines', ['bing', 'google'])
        
        # æœç´¢é…ç½®å‚æ•°
        search_config = searxng_config.get('search_config', {})
        self.max_results_per_query = search_config.get('max_results_per_query', 10)
        self.token_limit = search_config.get('token_limit', 3000)  # é»˜è®¤3000 tokens
        
        self.logger.info("ğŸ”§ RelicSearchToolkit åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“‚ é»˜è®¤æœç´¢åˆ†ç±»: {self.default_categories}")
        self.logger.info(f"ğŸ”§ é»˜è®¤æœç´¢å¼•æ“: {self.default_engines}")
    
    def search_relics(self, query: str, max_results: int = None) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡ç‰©ç›¸å…³ä¿¡æ¯
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
            if max_results is None:
                max_results = self.max_results_per_query
            
            # æ‰“å°å·¥å…·åŒ…æœç´¢è°ƒè¯•ä¿¡æ¯
            self.logger.info("ğŸ›ï¸ å¼€å§‹æ‰§è¡Œæ–‡ç‰©æœç´¢")
            self.logger.info(f"ğŸ“ åŸå§‹æŸ¥è¯¢: {query}")
            self.logger.info(f"ğŸ“Š æœ€å¤§ç»“æœæ•°: {max_results}")
            
            # å¢å¼ºæŸ¥è¯¢è¯ï¼Œæ·»åŠ æ–‡ç‰©ç›¸å…³å…³é”®è¯
            enhanced_query = self._enhance_query(query)
            self.logger.info(f"ğŸ” å¢å¼ºåæŸ¥è¯¢: {enhanced_query}")
            
            # æ‰§è¡Œæœç´¢
            results = self.searxng.search(
                query=enhanced_query,
                categories=self.default_categories,
                engines=self.default_engines,
                max_results=max_results
            )
            
            self.logger.info(f"âœ… æœç´¢å®Œæˆï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            formatted_results = [
                {
                    'title': result.title,
                    'url': result.url,
                    'content': result.content,
                    'score': result.score,
                    'source': result.source
                }
                for result in results
            ]
            
            # æ‰“å°ç»“æœæ‘˜è¦
            self.logger.info("ğŸ“‹ æœç´¢ç»“æœæ‘˜è¦:")
            for i, result in enumerate(formatted_results[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç»“æœ
                self.logger.info(f"  {i+1}. {result['title'][:50]}... (è¯„åˆ†: {result['score']:.2f})")
            
            if len(formatted_results) > 5:
                self.logger.info(f"  ... è¿˜æœ‰ {len(formatted_results) - 5} ä¸ªç»“æœ")
            
            return formatted_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡ç‰©æœç´¢å¤±è´¥: {str(e)}")
            return [{'error': f"æœç´¢å¤±è´¥: {str(e)}"}]
    
    def extract_page_content(self, url: str) -> Dict[str, str]:
        """
        æå–ç½‘é¡µè¯¦ç»†å†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            ç½‘é¡µå†…å®¹å­—å…¸
        """
        self.logger.info(f"ğŸ“„ å¼€å§‹æå–ç½‘é¡µå†…å®¹: {url}")
        result = self.extractor.extract_content(url)
        self.logger.info(f"âœ… ç½‘é¡µå†…å®¹æå–å®Œæˆ")
        return result
    
    def _enhance_query(self, query: str) -> str:
        """å¢å¼ºæœç´¢æŸ¥è¯¢ï¼Œæ·»åŠ æ–‡ç‰©ç›¸å…³å…³é”®è¯"""
        relic_keywords = ["æ–‡ç‰©", "å¤ä»£", "å†å²", "åšç‰©é¦†", "æ–‡åŒ–", "è‰ºæœ¯"]
        
        # å¦‚æœæŸ¥è¯¢ä¸­æ²¡æœ‰æ–‡ç‰©ç›¸å…³å…³é”®è¯ï¼Œåˆ™æ·»åŠ 
        if not any(keyword in query for keyword in relic_keywords):
            query = f"{query} æ–‡ç‰©"
        
        return query
    
    def _format_search_results_for_llm(self, results: List[Dict[str, Any]], max_tokens: int = None) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œæ§åˆ¶è¿”å›ç»™LLMçš„å†…å®¹é•¿åº¦
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆç²—ç•¥ä¼°ç®—ï¼Œ1tokençº¦4ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
            
        Returns:
            æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
        """
        if not results:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚"
        
        # ä½¿ç”¨é…ç½®çš„tokené™åˆ¶
        if max_tokens is None:
            max_tokens = self.token_limit
        
        # ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4ä¸ªä¸­æ–‡å­—ç¬¦ â‰ˆ 3ä¸ªè‹±æ–‡å­—ç¬¦
        max_chars = max_tokens * 3
        formatted_results = []
        current_length = 0
        
        # æ·»åŠ å¤´éƒ¨ä¿¡æ¯
        header = f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡ç‰©ä¿¡æ¯ï¼š\n\n"
        formatted_results.append(header)
        current_length += len(header)
        
        for i, result in enumerate(results[:8]):  # æœ€å¤šå¤„ç†å‰8ä¸ªç»“æœï¼Œè¿›ä¸€æ­¥å‡å°‘
            # æ™ºèƒ½æˆªæ–­æ ‡é¢˜å’Œå†…å®¹ï¼Œä¿ç•™å…³é”®ä¿¡æ¯
            title = self._extract_key_info(result.get('title', 'æ— æ ‡é¢˜'), 80)
            content = self._extract_key_info(result.get('content', 'æ— å†…å®¹'), 150)
            url = result.get('url', '')
            score = result.get('score', 0)
            
            # æ„å»ºå•ä¸ªç»“æœçš„å­—ç¬¦ä¸²ï¼Œç®€åŒ–æ ¼å¼
            result_str = f"{i+1}. {title}\n"
            result_str += f"   {content}\n"
            if score > 0.5:  # åªæ˜¾ç¤ºé«˜ç›¸å…³æ€§çš„æ¥æº
                result_str += f"   æ¥æºï¼š{self._shorten_url(url)}\n"
            result_str += "\n"
            
            # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºé•¿åº¦é™åˆ¶
            if current_length + len(result_str) > max_chars:
                # å¦‚æœæ·»åŠ å½“å‰ç»“æœä¼šè¶…å‡ºé™åˆ¶ï¼Œåˆ™åœæ­¢æ·»åŠ 
                remaining_count = len(results) - i
                if remaining_count > 0:
                    formatted_results.append(f"... è¿˜æœ‰ {remaining_count} ä¸ªç»“æœå› é•¿åº¦é™åˆ¶æœªæ˜¾ç¤º")
                break
            
            formatted_results.append(result_str)
            current_length += len(result_str)
        
        result_text = ''.join(formatted_results)
        
        # è®°å½•æˆªæ–­ä¿¡æ¯
        self.logger.info(f"ğŸ”¤ æœç´¢ç»“æœæ ¼å¼åŒ–å®Œæˆï¼šåŸå§‹ {len(results)} ä¸ªç»“æœï¼Œè¿”å›å†…å®¹é•¿åº¦ {len(result_text)} å­—ç¬¦")
        
        return result_text
    
    def _extract_key_info(self, text: str, max_length: int) -> str:
        """
        æ™ºèƒ½æå–å…³é”®ä¿¡æ¯ï¼Œä¼˜å…ˆä¿ç•™æ–‡ç‰©ç›¸å…³å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            ä¼˜åŒ–åçš„æ–‡æœ¬
        """
        if not text or len(text) <= max_length:
            return text
        
        # æ–‡ç‰©ç›¸å…³å…³é”®è¯
        important_keywords = [
            'æ–‡ç‰©', 'å¤è¿¹', 'é—å€', 'åšç‰©é¦†', 'å†å²', 'æ–‡åŒ–', 'æ”¶è—', 'å±•å“',
            'æœä»£', 'å¹´ä»£', 'è€ƒå¤', 'å¤ä»£', 'æ–‡çŒ®', 'è‰ºæœ¯', 'ä¼ ç»Ÿ', 'ä¿æŠ¤',
            'ä¸–ç•Œé—äº§', 'å›½å®', 'çå“', 'å¤è‘£', 'å…¸è—', 'é™¶ç“·', 'é’é“œ', 'ç‰å™¨'
        ]
        
        # æŒ‰å¥å·åˆ†å¥
        sentences = text.split('ã€‚')
        result_sentences = []
        current_length = 0
        
        # ä¼˜å…ˆé€‰æ‹©åŒ…å«å…³é”®è¯çš„å¥å­
        for sentence in sentences:
            if current_length >= max_length:
                break
            
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # æ£€æŸ¥å¥å­æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
            has_keyword = any(keyword in sentence for keyword in important_keywords)
            
            if has_keyword or len(result_sentences) == 0:  # ä¿è¯è‡³å°‘æœ‰ä¸€å¥
                if current_length + len(sentence) <= max_length:
                    result_sentences.append(sentence)
                    current_length += len(sentence)
                else:
                    # æˆªæ–­å½“å‰å¥å­
                    remaining = max_length - current_length
                    if remaining > 10:  # ä¿è¯æœ‰æ„ä¹‰çš„æˆªæ–­
                        result_sentences.append(sentence[:remaining] + "...")
                    break
        
        return 'ã€‚'.join(result_sentences) if result_sentences else text[:max_length] + "..."
    
    def _shorten_url(self, url: str) -> str:
        """
        ç¼©çŸ­URLæ˜¾ç¤º
        
        Args:
            url: åŸå§‹URL
            
        Returns:
            ç¼©çŸ­çš„URL
        """
        if not url:
            return ""
        
        # æå–åŸŸå
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc
            if domain:
                return domain
        except:
            pass
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œç®€å•æˆªæ–­
        if len(url) > 50:
            return url[:47] + "..."
        return url
    
    def _format_content_for_llm(self, content_result: Dict[str, Any], max_tokens: int = 2000) -> str:
        """
        æ ¼å¼åŒ–ç½‘é¡µå†…å®¹ï¼Œæ§åˆ¶è¿”å›ç»™LLMçš„å†…å®¹é•¿åº¦
        
        Args:
            content_result: ç½‘é¡µå†…å®¹ç»“æœ
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            æ ¼å¼åŒ–çš„å†…å®¹å­—ç¬¦ä¸²
        """
        if not content_result or 'error' in content_result:
            return f"ç½‘é¡µå†…å®¹æå–å¤±è´¥ï¼š{content_result.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        max_chars = max_tokens * 3
        title = content_result.get('title', 'æ— æ ‡é¢˜')[:100]
        content = content_result.get('content', 'æ— å†…å®¹')
        url = content_result.get('url', '')
        
        # æ„å»ºç»“æœå­—ç¬¦ä¸²
        result_str = f"æ ‡é¢˜ï¼š{title}\n"
        result_str += f"æ¥æºï¼š{url}\n"
        result_str += f"å†…å®¹ï¼š\n"
        
        # è®¡ç®—å†…å®¹å¯ç”¨çš„å­—ç¬¦æ•°
        available_chars = max_chars - len(result_str)
        
        if len(content) > available_chars:
            content = content[:available_chars] + "...[å†…å®¹å› é•¿åº¦é™åˆ¶è¢«æˆªæ–­]"
        
        result_str += content
        
        # è®°å½•æˆªæ–­ä¿¡æ¯
        self.logger.info(f"ğŸ”¤ ç½‘é¡µå†…å®¹æ ¼å¼åŒ–å®Œæˆï¼šè¿”å›å†…å®¹é•¿åº¦ {len(result_str)} å­—ç¬¦")
        
        return result_str
    
    def get_langchain_tools(self) -> List[Tool]:
        """è·å–LangChainå·¥å…·åˆ—è¡¨"""
        tools = [
            Tool(
                name="search_relics",
                description="æœç´¢æ–‡ç‰©ç›¸å…³ä¿¡æ¯ã€‚è¾“å…¥ï¼šæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ã€‚è¾“å‡ºï¼šç›¸å…³æ–‡ç‰©ä¿¡æ¯çš„æœç´¢ç»“æœåˆ—è¡¨ã€‚",
                func=lambda query: self._format_search_results_for_llm(self.search_relics(query))
            ),
            Tool(
                name="extract_content",
                description="æå–ç½‘é¡µçš„è¯¦ç»†å†…å®¹ã€‚è¾“å…¥ï¼šç½‘é¡µURLã€‚è¾“å‡ºï¼šç½‘é¡µçš„æ ‡é¢˜å’Œä¸»è¦å†…å®¹ã€‚",
                func=lambda url: self._format_content_for_llm(self.extract_page_content(url))
            )
        ]
        
        return tools
